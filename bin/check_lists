#!/usr/bin/env ruby

require 'bundler/setup'
require 'thread'
require 'optparse'
require_relative '../lib/url_categorise'

# Default configuration
options = {
  parallel_enabled: false,
  max_threads: 8,
  verbose: false
}

# Parse command-line options
OptionParser.new do |opts|
  opts.banner = "Usage: #{$0} [options]"
  opts.separator ''
  opts.separator 'Check all URLs in constants for availability'
  opts.separator ''

  opts.on('-p', '--parallel', 'Enable parallel URL checking') do
    options[:parallel_enabled] = true
  end

  opts.on('-t', '--threads NUM', Integer, "Number of threads to use (default: #{options[:max_threads]})") do |num|
    options[:max_threads] = num
  end

  opts.on('-v', '--verbose', 'Verbose output') do
    options[:verbose] = true
  end

  opts.on('-h', '--help', 'Show this help message') do
    puts opts
    exit
  end
end.parse!

puts '=== CHECKING ALL URLs IN CONSTANTS ==='
puts "ğŸš€ Using parallel processing with #{options[:max_threads]} threads" if options[:parallel_enabled]

def check_url(url)
  begin
    response = HTTParty.head(url, timeout: 10)
    case response.code
    when 200
      { url: url, status: 'âœ… OK', code: response.code }
    when 404
      { url: url, status: 'âŒ 404 Not Found', code: response.code }
    when 403
      { url: url, status: 'âŒ 403 Forbidden', code: response.code }
    when 500..599
      { url: url, status: "âŒ Server Error (#{response.code})", code: response.code }
    else
      { url: url, status: "âš ï¸ HTTP #{response.code}", code: response.code }
    end
  rescue Net::TimeoutError, HTTParty::TimeoutError
    { url: url, status: 'âŒ Timeout', code: nil }
  rescue SocketError, Errno::ECONNREFUSED
    { url: url, status: 'âŒ DNS/Network Error', code: nil }
  rescue StandardError => e
    { url: url, status: "âŒ Error: #{e.class}", code: nil }
  end
end

def check_urls_parallel(urls, max_threads)
  results = []
  results_mutex = Mutex.new
  
  # Create work queue
  queue = Queue.new
  urls.each { |url| queue << url }
  
  # Create worker threads
  threads = []
  max_threads.times do
    threads << Thread.new do
      while !queue.empty?
        begin
          url = queue.pop(true) # non-blocking pop
          result = check_url(url)
          
          results_mutex.synchronize do
            results << result
          end
        rescue ThreadError
          # Queue is empty, thread can exit
          break
        end
      end
    end
  end
  
  # Wait for all threads to complete
  threads.each(&:join)
  
  # Sort results to maintain original order
  urls.map { |url| results.find { |r| r[:url] == url } }
end

def check_urls_sequential(urls)
  urls.map do |url|
    print "  Testing #{url}... "
    result = check_url(url)
    puts result[:status]
    result
  end
end

UrlCategorise::Constants::DEFAULT_HOST_URLS.each do |category, urls|
  puts "\n#{category.upcase}:"

  # Skip categories that only reference other categories (symbols)
  actual_urls = urls.reject { |url| url.is_a?(Symbol) }

  if actual_urls.empty?
    if urls.empty?
      puts '  Empty category (no URLs defined)'
    else
      puts "  Only references other categories: #{urls}"
    end
    next
  end

  if options[:parallel_enabled] && actual_urls.length > 1
    puts "  âš¡ Checking #{actual_urls.length} URLs in parallel..."
    start_time = Time.now
    results = check_urls_parallel(actual_urls, [options[:max_threads], actual_urls.length].min)
    end_time = Time.now
    
    # Display results
    results.each do |result|
      puts "  #{result[:url]} â†’ #{result[:status]}"
    end
    
    puts "  â±ï¸ Completed in #{'%.2f' % ((end_time - start_time) * 1000)}ms"
  else
    results = check_urls_sequential(actual_urls)
  end
  
  # Summary for this category
  ok_count = results.count { |r| r[:status].include?('âœ…') }
  error_count = results.length - ok_count
  puts "  ğŸ“Š Summary: #{ok_count} OK, #{error_count} errors" if results.length > 1
end

puts "\nğŸ¯ Health check completed!"
puts "ğŸ’¡ Tip: Use --parallel (-p) for faster checking and --threads (-t) NUM to control thread count"
